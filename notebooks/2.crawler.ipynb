{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4.element import Tag\n",
    "from bs4 import BeautifulSoup\n",
    "from news_summarizer.webdriver import WebDriverFactory, ShutilBrowserLocator\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from abc import abstractmethod\n",
    "from pydantic import HttpUrl\n",
    "from datetime import datetime\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake the Mongo database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeMongoCollection:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def insert_one(self, document):\n",
    "        if \"_id\" not in document:\n",
    "            raise ValueError(\"Document must contain an '_id' field\")\n",
    "        if document[\"_id\"] in self.data:\n",
    "            raise ValueError(\"Duplicate _id found\")\n",
    "        self.data[document[\"_id\"]] = document\n",
    "        print(f\"Inserted document: {document}\")\n",
    "\n",
    "    def insert_many(self, documents):\n",
    "        if not documents:\n",
    "            raise ValueError(\"Documents list cannot be empty\")\n",
    "        for document in documents:\n",
    "            if \"_id\" not in document:\n",
    "                raise ValueError(\"Each document must contain an '_id' field\")\n",
    "            if document[\"_id\"] in self.data:\n",
    "                raise ValueError(\"Duplicate _id found\")\n",
    "            self.data[document[\"_id\"]] = document\n",
    "        print(f\"Inserted documents: {documents}\")\n",
    "\n",
    "    def find_one(self, query):\n",
    "        for document in self.data.values():\n",
    "            if all(\n",
    "                self._match_query(document, key, value) for key, value in query.items()\n",
    "            ):\n",
    "                print(f\"Found document: {document}\")\n",
    "                return document\n",
    "        print(\"No document found\")\n",
    "        return None\n",
    "\n",
    "    def find(self, query):\n",
    "        results = [\n",
    "            document\n",
    "            for document in self.data.values()\n",
    "            if all(\n",
    "                self._match_query(document, key, value) for key, value in query.items()\n",
    "            )\n",
    "        ]\n",
    "        print(f\"Found documents: {results}\")\n",
    "        return results\n",
    "\n",
    "    def _match_query(self, document, key, value):\n",
    "        if isinstance(value, dict) and \"$regex\" in value:\n",
    "            return re.search(value[\"$regex\"], document.get(key, \"\")) is not None\n",
    "        return document.get(key) == value\n",
    "\n",
    "\n",
    "class FakeDatabase:\n",
    "    def __init__(self):\n",
    "        self.collections = {}\n",
    "\n",
    "    def __getitem__(self, collection_name: str) -> FakeMongoCollection:\n",
    "        # Automatically create a collection if it doesn't exist\n",
    "        if collection_name not in self.collections:\n",
    "            self.collections[collection_name] = FakeMongoCollection()\n",
    "        return self.collections[collection_name]\n",
    "\n",
    "    def __setitem__(self, collection_name: str, collection: FakeMongoCollection):\n",
    "        self.collections[collection_name] = collection\n",
    "\n",
    "\n",
    "class FakeMongoClient:\n",
    "    def __init__(self):\n",
    "        self.databases = {}\n",
    "\n",
    "    def __getitem__(self, db_name: str) -> FakeDatabase:\n",
    "        # Automatically create a database if it doesn't exist\n",
    "        if db_name not in self.databases:\n",
    "            self.databases[db_name] = FakeDatabase()\n",
    "        return self.databases[db_name]\n",
    "\n",
    "    def __setitem__(self, db_name: str, database: FakeDatabase):\n",
    "        self.databases[db_name] = database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from abc import ABC\n",
    "from pydantic import UUID4, BaseModel, Field\n",
    "from typing import Generic, Type, TypeVar, Dict, List\n",
    "from news_summarizer.database.mongo import fake_connection\n",
    "\n",
    "_database = fake_connection[\"null_database\"]\n",
    "\n",
    "T = TypeVar(\"T\", bound=\"NoSQLBaseLink\")\n",
    "\n",
    "class NoSQLBaseLink(BaseModel, Generic[T], ABC):\n",
    "    id: UUID4 = Field(default_factory=uuid.uuid4)\n",
    "\n",
    "    def __eq__(self, value: object) -> bool:\n",
    "        if not isinstance(value, self.__class__):\n",
    "            return False\n",
    "        return self.id == value.id\n",
    "    \n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.id)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_mongo(cls: Type[T], data:Dict) -> T:\n",
    "        if not data:\n",
    "            raise ValueError(\"Data is empty.\")\n",
    "        \n",
    "        id = data.pop(\"_id\")\n",
    "\n",
    "        return cls(**dict(data, id=id))\n",
    "    \n",
    "    def to_mongo(self: T, **kwargs) -> Dict:\n",
    "        exclude_unset = kwargs.pop(\"exclude_unset\", False)\n",
    "        by_alias = kwargs.pop(\"by_alias\", True)\n",
    "\n",
    "        parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n",
    "\n",
    "        if \"_id\" not in parsed and \"id\" in parsed:\n",
    "            parsed[\"_id\"] = str(parsed.pop(\"id\"))\n",
    "\n",
    "        for key, value in parsed.items():\n",
    "            if isinstance(value, uuid.UUID):\n",
    "                parsed[key] = str(value)\n",
    "\n",
    "\n",
    "        return parsed\n",
    "    \n",
    "    def model_dump(self: T, **kwargs) -> Dict:\n",
    "        dict_ = super().model_dump(**kwargs)\n",
    "\n",
    "        for key, value in dict_.items():\n",
    "            if isinstance(value, uuid.UUID):\n",
    "                dict_[key] = str(value)\n",
    "\n",
    "        return dict_\n",
    "    \n",
    "    def save(self: T, **kwargs) -> T | None:\n",
    "        collection = _database[self.get_collection_name()]\n",
    "        try:\n",
    "            collection.insert_one(self.to_mongo(**kwargs))\n",
    "            return self\n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "    @classmethod\n",
    "    def get_or_create(cls: Type[T], **filter_options) -> T:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instance = collection.find_one(filter_options)\n",
    "            if instance:\n",
    "                return cls.from_mongo(instance)\n",
    "\n",
    "            new_instance = cls(**filter_options)\n",
    "            new_instance = new_instance.save()\n",
    "\n",
    "            return new_instance\n",
    "        except Exception:\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    def bulk_insert(cls: Type[T], links: List[T], **kwargs) -> bool:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            collection.insert_many(link.to_mongo(**kwargs) for link in links)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def find(cls: Type[T], **filter_options) -> T | None:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instance = collection.find_one(filter_options)\n",
    "            if instance:\n",
    "                return cls.from_mongo(instance)\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def bulk_find(cls: Type[T], **filter_options) -> List[T]:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instances = collection.find(filter_options)\n",
    "            return [link for instance in instances if (link := cls.from_mongo(instance)) is not None]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def get_collection_name(cls: Type[T]) -> str:\n",
    "        if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return cls.Settings.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Link(NoSQLBaseLink):\n",
    "    title: str = Field(..., description=\"The title of the link\")\n",
    "    url: HttpUrl = Field(..., description=\"The URL of the link\")\n",
    "    source: Optional[str] = Field(None, description=\"The source of the link\")\n",
    "    published_at: Optional[datetime] = Field(None, description=\"The publication date of the link\")\n",
    "    extracted_at: datetime = Field(default_factory=datetime.now, description=\"The timestamp when the link was extracted\")\n",
    "\n",
    "    class Settings:\n",
    "        name = \"link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCrawler(ABC):\n",
    "    @abstractmethod\n",
    "    def search(self, link: str, **kwargs) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BaseSeleniumCrawler(BaseCrawler, ABC):\n",
    "    def __init__(self, scroll_limit: int = 5) -> None:\n",
    "        self.driver = WebDriverFactory(ShutilBrowserLocator()).get_webdriver()\n",
    "        self.scroll_limit = scroll_limit\n",
    "        self.soup = None\n",
    "\n",
    "def extract_date_from_url(url: str) -> str:\n",
    "    # Regular expression to match the date in the format YYYY/MM/DD\n",
    "    match = re.search(r\"(\\d{4}/\\d{2}/\\d{2})\", url)\n",
    "\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        # Convert the date string to a datetime object\n",
    "        return datetime.strptime(date_str, \"%Y/%m/%d\")\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_title(url: str) -> str:\n",
    "    last_segment = url.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "    # Remove HTML-like extensions\n",
    "    last_segment = re.sub(r\"\\.html?|\\.htm|\\.ghtml$\", \"\", last_segment)\n",
    "\n",
    "    # Replace separators (-, _, etc.) with spaces and convert to lowercase\n",
    "    title = re.sub(r\"[-_]\", \" \", last_segment)\n",
    "\n",
    "    # Optional: Replace multiple spaces with a single space\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "\n",
    "    return title\n",
    "\n",
    "def extract_links(elements: List[Tag]):\n",
    "    data = []\n",
    "    for element in elements:\n",
    "        url = element.get(\"href\")\n",
    "\n",
    "        title = element.text\n",
    "        if len(title) < 5:\n",
    "            title = extract_title(url)\n",
    "\n",
    "        published_at = extract_date_from_url(url)\n",
    "\n",
    "        link = {\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"published_at\": published_at,\n",
    "        }\n",
    "\n",
    "        data.append(link)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class G1Crawler(BaseSeleniumCrawler):\n",
    "    model = Link\n",
    "\n",
    "    def __init__(self, scroll_limit: int = 5) -> None:\n",
    "        super().__init__(scroll_limit=scroll_limit)\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        load_mode = 0\n",
    "        page_number = 0\n",
    "        last_page_number = 0\n",
    "\n",
    "        while True:\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "            time.sleep(np.random.randint(2, 5))\n",
    "            # Wait for the \"Veja mais\" link to appear with the next page number\n",
    "            try:\n",
    "\n",
    "                load_more_link = WebDriverWait(self.driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"div.load-more a\"))\n",
    "                )\n",
    "\n",
    "                url = load_more_link.get_dom_attribute(\"href\")\n",
    "                page_number = self._extract_page_number(url)\n",
    "\n",
    "                if page_number > last_page_number:\n",
    "                    load_mode += 1\n",
    "                    last_page_number = page_number\n",
    "\n",
    "                    if load_mode >= 6:\n",
    "                        break\n",
    "                load_more_link.click()\n",
    "            except Exception as e:\n",
    "                print(\"see more link not found yet, scrolling more...\")\n",
    "\n",
    "    def _extract_page_number(self, url):\n",
    "        match = re.search(r\"pagina-(\\d+)\", url)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    def search(self, link: str, **kwargs) -> None:\n",
    "        self.driver.get(link)\n",
    "        time.sleep(5)\n",
    "        self.scroll_page()\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        elements = soup.find_all(\"a\", href=True)\n",
    "        hyperlinks = extract_links(elements)\n",
    "        self.driver.close()\n",
    "\n",
    "        hyperlink_list = []\n",
    "        for hyperlink in hyperlinks:\n",
    "            try:\n",
    "                hyperlink_list.append(Link(title=hyperlink[\"title\"], url=hyperlink[\"url\"], source=link, published_at=hyperlink[\"published_at\"]))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "            \n",
    "        self.model.bulk_insert(hyperlink_list)\n",
    "\n",
    "class BandCrawler(BaseSeleniumCrawler):\n",
    "    model = Link\n",
    "\n",
    "    def __init__(self, scroll_limit: int = 2) -> None:\n",
    "        super().__init__(scroll_limit=scroll_limit)\n",
    "        self.links = None\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        current_scroll = 0\n",
    "        while True:\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height or (\n",
    "                self.scroll_limit and current_scroll >= self.scroll_limit\n",
    "            ):\n",
    "                break\n",
    "            last_height = new_height\n",
    "            current_scroll += 1\n",
    "\n",
    "    def search(self, link: str, **kwargs) -> None:\n",
    "        self.driver.get(link)\n",
    "        time.sleep(5)\n",
    "        self.scroll_page()\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        elements = soup.find_all(\"a\", href=True)\n",
    "        hyperlinks = extract_links(elements)\n",
    "        self.driver.close()\n",
    "\n",
    "        hyperlink_list = []\n",
    "        for hyperlink in hyperlinks:\n",
    "            try:\n",
    "                hyperlink_list.append(Link(title=hyperlink[\"title\"], url=hyperlink[\"url\"], source=link, published_at=hyperlink[\"published_at\"]))\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "            \n",
    "        self.model.bulk_insert(hyperlink_list)\n",
    "\n",
    "class R7Crawler(BaseSeleniumCrawler):\n",
    "    model = Link\n",
    "    \n",
    "    def __init__(self, scroll_limit: int = 2) -> None:\n",
    "        super().__init__(scroll_limit=scroll_limit)\n",
    "        self.links = None\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        current_scroll = 0\n",
    "        while True:\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height or (\n",
    "                self.scroll_limit and current_scroll >= self.scroll_limit\n",
    "            ):\n",
    "                break\n",
    "            last_height = new_height\n",
    "            current_scroll += 1\n",
    "\n",
    "    def search(self, link: str, **kwargs) -> None:\n",
    "        self.driver.get(link)\n",
    "        time.sleep(5)\n",
    "        self.scroll_page()\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        elements = soup.find_all(\"a\", href=True)\n",
    "        hyperlinks = extract_links(elements)\n",
    "        self.driver.close()\n",
    "\n",
    "        hyperlink_list = []\n",
    "        for hyperlink in hyperlinks:\n",
    "            try:\n",
    "                hyperlink_list.append(Link(title=hyperlink[\"title\"], url=hyperlink[\"url\"], source=link, published_at=hyperlink[\"published_at\"]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "        self.model.bulk_insert(hyperlink_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g1_crawler = G1Crawler()\n",
    "#g1_crawler.search(link='https://g1.globo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band_crawler = BandCrawler()\n",
    "#band_crawler.search(link='https://bandnewstv.uol.com.br')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r7_crawler = R7Crawler()\n",
    "#r7_crawler.search(link='https://www.r7.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "class CrawlerRegistry:\n",
    "    def __init__(self):\n",
    "        self._crawlers = {}\n",
    "\n",
    "    def register(self, name, crawler: BaseCrawler):\n",
    "        if name in self._crawlers:\n",
    "            raise ValueError(\"Component '%s' is already registered.\", name)\n",
    "        self._crawlers[name] = crawler\n",
    "\n",
    "    def get(self, name):\n",
    "        parsed_domain = urlparse(name)\n",
    "        name = self._extract_netloc(parsed_domain)\n",
    "\n",
    "        if name not in self._crawlers:\n",
    "            raise KeyError(\"Component '%s' not found.\")\n",
    "        return self._crawlers[name]()\n",
    "\n",
    "    def _extract_netloc(self, domain):\n",
    "        return f\"{domain.scheme}://{domain.netloc}/\"\n",
    "\n",
    "    def list_crawlers(self):\n",
    "        return list(self._components.keys())\n",
    "\n",
    "    \n",
    "registry = CrawlerRegistry()\n",
    "registry.register(\"http://g1.globo.com/\", G1Crawler)\n",
    "registry.register(\"https://www.r7.com/\", R7Crawler)\n",
    "registry.register('https://bandnewstv.uol.com.br/', BandCrawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_crawler = registry.get(\"http://g1.globo.com/noticia-dia-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see more link not found yet, scrolling more...\n"
     ]
    }
   ],
   "source": [
    "g1_crawler.search(\"http://g1.globo.com/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
