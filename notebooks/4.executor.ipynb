{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_summarizer.domain.base.nosql import _database\n",
    "\n",
    "collection_link = _database.get_collection(\"link\")\n",
    "indexes = collection_link.find({\"source\": \"https://g1.globo.com/\"})\n",
    "indexes = list(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_summarizer.crawler.registry import crawler_registry\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from news_summarizer.crawler.registry import CrawlerRegistry\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CrawlerExecutor:\n",
    "    def __init__(self, crawler_registry: CrawlerRegistry):\n",
    "        self.crawler_registry = crawler_registry\n",
    "\n",
    "    def run(self, links: List[str]):\n",
    "        with ThreadPoolExecutor(max_workers=len(links)) as executor:\n",
    "            # Map each link to its corresponding crawler using the registry\n",
    "            futures = [executor.submit(self._run_crawler, link) for link in links]\n",
    "            for future in futures:\n",
    "                try:\n",
    "                    future.result()  # Wait for the crawler to complete\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error occurred during crawling: {e}\")\n",
    "\n",
    "    def _run_crawler(self, link: str):\n",
    "        logger.info(f\"Starting crawler for link: {link}\")\n",
    "\n",
    "        # Use the registry to select the appropriate crawler based on the link\n",
    "        crawler_cls = self.crawler_registry.get(link)\n",
    "        if not crawler_cls:\n",
    "            logger.error(f\"No crawler registered for link: {link}\")\n",
    "            return\n",
    "\n",
    "        crawler = crawler_cls()\n",
    "        crawler.search(link)\n",
    "        logger.info(f\"Finished crawling for link: {link}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = [\n",
    "        \"https://www.r7.com/\",\n",
    "        \"https://g1.globo.com/\",\n",
    "        \"https://bandnewstv.uol.com.br/\",\n",
    "    ]\n",
    "\n",
    "    # Initialize CrawlerExecutor with the registry\n",
    "    executor = CrawlerExecutor(crawler_registry)\n",
    "    executor.run(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
